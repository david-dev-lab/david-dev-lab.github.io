---
layout: ../../layouts/ProjectLayout.astro
title: 从SFT到RL
description: LLM后训练流水线中的阶段切换时机与决策依据
tags: ["LLM", "SFT", "RLHF", "Post-Training"]
---

# 从SFT到RL：何时切换训练范式？

后训练阶段的核心矛盾在于：SFT能教会模型"照着做"，却无法让它学会"做得更好"。理解两者的本质差异，是判断切换时机的关键。

## 1. 两种范式的本质

SFT的学习目标是最小化与标注答案的token级别差异——本质上是**模仿学习**。这决定了它的能力天花板：无论训练多久，模型最多只能达到标注数据的平均水平。

RL则引入了一个外部评判者（Reward Model或规则校验器），模型不再追求"和答案一样"，而是追求"让评判者满意"。这种**序列级别的反馈**允许模型探索标注数据之外的解空间。

| 维度 | SFT | RL |
|:---|:---|:---|
| 优化信号 | 逐token交叉熵 | 整体序列奖励 |
| 学习上界 | 数据集质量 | 奖励函数设计 |
| 核心价值 | 建立基础能力 | 突破性能边界 |

## 2. SFT的退出信号

以下现象表明SFT已接近其能力边界：

**Loss曲线失去指导意义**：验证集loss持续下降，但人工评估或benchmark分数停滞。这说明模型在学习数据集的统计规律，而非真正提升能力。

**格式能力已稳定**：模型能可靠地遵循指令格式（JSON输出、多轮对话、拒绝越界请求）。SFT的首要任务是让预训练模型"听懂人话"，一旦达成，继续训练的边际收益急剧下降。

**Pass@k远高于Pass@1**：在代码或数学任务上，让模型生成k次取最优的通过率，显著高于单次生成。这个gap本质上是RL可以挖掘的潜力空间——模型"知道"正确答案，只是第一次不一定能给出。

## 3. RL的适用场景

并非所有任务都需要RL。以下场景是RL真正发挥价值的地方：

### 3.1 偏好对齐

当"好"与"坏"难以用规则定义时，需要通过人类偏好数据训练Reward Model。典型场景：回答的有用性、文风的专业度、拒绝有害请求的方式。

### 3.2 可验证的推理任务

代码能否通过单元测试、数学答案是否正确——这类任务有天然的奖励信号。RL可以直接利用验证结果作为reward，无需人工标注。

### 3.3 抑制不良行为

SFT通过正例教模型"该说什么"，但很难教"不该说什么"。RL的惩罚机制能有效降低幻觉、有害输出、格式违规等问题的发生概率。

## 4. 工程实践中的迭代模式

实际训练很少是线性的SFT→RL单向流程，更常见的是螺旋式迭代：

```
SFT (冷启动) → RL (能力突破) → Rejection Sampling → SFT (蒸馏) → RL ...
```

**冷启动SFT**：用少量高质量数据（通常几千到几万条）让模型具备基本的指令遵循能力。

**RL优化**：针对目标任务进行强化学习，可能是RLHF、DPO，或基于规则的奖励。

**知识蒸馏**：收集RL阶段模型生成的高质量输出，筛选后作为新的SFT数据。这一步将RL探索到的"好答案"固化为模型的默认行为。

这种迭代的本质是：用RL探索能力边界，用SFT稳定已验证的能力。
