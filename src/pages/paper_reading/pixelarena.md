---
layout: ../../layouts/ProjectLayout.astro
title: PixelArena
description: 针对多模态模型像素级视觉生成能力的评估基准
tags: ["OMM", "Semantic Segmentation", "Benchmark"]
---

# PixelArena: A Benchmark for Pixel-Precision Visual Intelligence

<p class="related-content" style="margin-top: 0.5rem; padding: 0.5rem 0.75rem; background: #f9fafb; border: 1px solid #e5e7eb; border-radius: 0.25rem;">
<strong>论文信息</strong>：<a href="https://arxiv.org/abs/2507.xxxxx" style="color: #2563eb;">arXiv</a> | 2025
</p>

本文针对多模态模型在多模态生成评估中的局限性，提出了一套基于语义分割任务的基准测试。作者认为当前的评估往往过于关注图像美学，而忽视了模型对像素级生成的精细控制能力和逻辑推理能力。

## 1. 核心问题与挑战

自GPT-4o发布以来，能够处理多模态输入输出的多模态模型成为研究热点，但对其生成能力的定量评估仍存在明显短板：

| 挑战 | 说明 |
|:---|:---|
| **主观性强** | 现有图像生成基准主要关注美学质量，依赖人类偏好或模型打分，存在隐性偏见 |
| **精度评估缺失** | 缺乏对模型细粒度控制能力和视觉推理限制的定量考察 |
| **任务泛化性未知** | 尚不清楚这些通用模型是否真正理解了图像生成任务，还是仅仅记住了训练数据 |

## 2. 核心方法：PixelArena基准框架

PixelArena的核心思想是利用**语义分割**任务来客观评估多模态模型的**像素级视觉智能**。

### 2.1 评估流程

不同于传统分割模型输出标签矩阵，PixelArena要求多模态模型直接生成彩色掩码图像。流程如下：

1. **输入**：原始图像 + 颜色编码调色板 + 文本指令
2. **生成**：模型在零样本设置下绘制分割掩码
3. **映射**：计算生成像素与预定义颜色编码的欧氏距离，将其转化为标签索引
4. **评估**：使用F1 Score、mIoU、Dice等客观指标与Ground Truth对比

<img src="/images/palette_standard_color.png" alt="调色板示例" style="max-width: 400px; display: block; margin: 0 auto;">

上图展示了CelebAMask-HQ数据集的标准颜色编码调色板。每个语义类别对应一种固定颜色，模型需要根据调色板生成精确的彩色掩码。

### 2.2 受测模型

| 类型 | 模型 | 说明 |
|:---|:---|:---|
| **OMMs** | **Gemini 3 Pro Image** | 表现最佳，展现出涌现的零样本分割能力 |
| OMMs | Gemini 2.5 Flash Image | 缺乏精确颜色控制，表现较差 |
| OMMs | GPT Image 1 | 理解部分任务，但存在幻觉 |
| OMMs | Emu 3.5 / Uni-MoE-2 | 基本无法理解任务或无法控制生成过程 |
| **Baseline** | SegFace / OneFormer | 针对特定任务训练的传统SOTA视觉模型，作为天花板参考 |

### 2.3 数据集构建

为验证泛化性，作者选取了两个难度不同的数据集子集，统一处理为$1024 \times 1024$分辨率：

| 数据集 | 任务 | 类别数 | 难度 |
|:---|:---|:---|:---|
| **CelebAMask-HQ** | 人脸解析 | 19 | 相对简单 |
| **COCO** | 全景分割转语义分割 | 144 | 极具挑战性 |

## 3. Prompt设计分析

PixelArena采用了一套包含视觉基础的Prompt策略：

| 技巧 | 说明 |
|:---|:---|
| **视觉调色板** | 直接将颜色编码对应的色块图作为输入的一部分，帮助模型建立颜色-语义映射 |
| **零样本约束** | 不提供任何参考示例，强制模型依靠自身理解执行任务 |
| **歧义消除** | 显式说明左/右是相对于图中人物而言，而非图片本身，以减少空间推理错误 |

## 4. 实验结果与核心发现

<img src="/images/f1_score.png" alt="F1 Score对比" style="max-width: 560px; display: block; margin: 0 auto;">

上图展示了各模型在CelebAMask-HQ数据集上的F1 Score，P表示尝试次数。专用模型SegFace以约0.88领先，Gemini 3 Pro在P=5时可达约0.6，而Emu35和UniMoE-2几乎无法完成任务。

<img src="/images/comparison_of_models.png" alt="模型生成对比" style="max-width: 480px; display: block; margin: 0 auto;">

生成结果的可视化对比印证了上述量化结论。Gemini 3 Pro与SegFace的输出与参考掩码高度一致，GPT Image 1存在明显颜色偏差，Emu35和UniMoE-2则完全无法理解任务。

综合来看，Gemini 3 Pro以F1=0.708显著优于其他多模态模型，展现出初步的精细控制能力，但仍落后于专用模型SegFace的F1=0.954。在更难的COCO数据集上，Gemini 3 Pro虽有下降但仍保持了一定的理解力，而其他多模态模型几乎完全失败。

**关键发现1：真实泛化而非记忆**

为验证Gemini 3 Pro是否只是背记了公开数据集的掩码，作者进行了一项实验：**打乱颜色编码**。

- **结果**：模型在颜色打乱后的表现不仅没有下降，反而提升了约10%
- **结论**：模型并未死记硬背，而是真正理解了任务指令和视觉对应的逻辑关系

**关键发现2：虚假反思**

通过检查Gemini 3 Pro的思维链，作者发现了严重的知行不一现象。模型在思维链中声称已经严格检查了约束条件，但生成的图像却存在明显错误。

**案例分析**：

- **眼睛左右颠倒**：模型生成的掩码中搞混了左右眼，但在思维链中却写道：「我已验证……包括关键的左右翻转规则是准确的」
- **幻觉识别**：在COCO数据集中，模型将包含栅栏的图错误地全图涂满代表「网」的颜色，却在思维链中称：「网的类别被正确地用洋红色表示」

这种盲目肯定表明模型的视觉感知模块与逻辑推理模块之间存在断裂，或者是经过RLHF训练后学会了表面上的反思形式，而未掌握实质的视觉验证能力。

## 5. 局限性与未来方向

| 方向 | 建议 |
|:---|:---|
| **数据修正** | 多模态模型生成的某些结果质量极高，甚至可用于修正现有数据集的标注错误 |
| **机制可解释性** | 针对颜色打乱后性能提升及虚假反思现象，需进一步研究其内部视觉推理机制 |
| **新指标设计** | F1和mIoU虽然客观，但有时不能完全反映掩码的视觉相似度，需要设计更符合生成式模型特性的指标 |

## 6. 总结

PixelArena表明，最先进的多模态模型已涌现出像素级生成控制能力。但视觉感知的不稳定性与推理中的虚假反思，仍是迈向真正视觉智能的核心障碍。
